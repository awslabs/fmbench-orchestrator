Content-Type: multipart/mixed; boundary="//"
MIME-Version: 1.0

--//
Content-Type: text/cloud-config; charset="us-ascii"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename="cloud-config.txt"

#cloud-config
cloud_final_modules:
- [scripts-user, always]

--//
Content-Type: text/x-shellscript; charset="us-ascii"
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename="userdata.txt"

#!/bin/bash
cd /home/ubuntu/

# Update and install Docker as root
apt-get update
apt-get install --reinstall docker.io -y
apt-get install -y docker-compose
docker compose version

# Switch to ubuntu user and run the following commands
sudo -u ubuntu -i bash << 'EOF'
# Download and install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env
# create and activate fmbench venv
uv venv $HOME/.fmbench_python312 --python 3.12
source $HOME/.fmbench_python312/bin/activate

# Set the flag for latest version
fmbench_latest="__fmbench_latest__"  # You can set this to "False" to use pip install
fmbench_repo="__fmbench_repo__"  # replaced with None or 
                                 # or https://github.com/aws-samples/foundation-model-benchmarking-tool.git
                                 # or a custom user provided repo

# Conditional installation based on fmbench_latest flag
if [ "$fmbench_latest" = "True" ]; then    
    # Clone the repository
    git clone $fmbench_repo
    
    # Change to the repository directory
    cd foundation-model-benchmarking-tool
    
    # Build fmbench and install
    uv build
    uv pip install -U dist/*.whl
else
    # Install fmbench directly from pip
    uv pip install -U fmbench
fi

sudo usermod -a -G docker $USER
newgrp docker

# Download content from S3 using the provided script
curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- /tmp
echo "__HF_TOKEN__" > /tmp/fmbench-read/scripts/hf_token.txt

# Add the conda environment activation and directory navigation to .bashrc
echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc


neuron="__neuron__"

# Check if neuron_ls is successful and neuron flag is set to True
if neuron-ls && [ "$neuron" = "True" ]; then
    # Download the Dockerfile for Triton
    curl -o ./Dockerfile_triton https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/src/fmbench/scripts/triton/Dockerfile_triton

    # Download the script that builds and pushes the Triton image locally
    curl -o build_and_push_triton.sh https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/src/fmbench/scripts/triton/build_and_push_triton.sh

    # Make the build and push script executable
    chmod +x build_and_push_triton.sh

    # Run the build and push script
    ./build_and_push_triton.sh
fi


# install triton inference server, install only if running on a GPU instance
if nvidia-smi; then
    cd ~
    git clone https://github.com/triton-inference-server/tensorrtllm_backend.git --branch v0.12.0
    # Update the submodules
    cd tensorrtllm_backend
    # Install git-lfs if needed
    sudo apt --fix-broken install -y
    sudo apt-get update -y && sudo apt-get install git-lfs -y --no-install-recommends
    git lfs install
    git submodule update --init --recursive
fi

source /home/ubuntu/.bashrc
touch /tmp/startup_complete.flag

EOF
